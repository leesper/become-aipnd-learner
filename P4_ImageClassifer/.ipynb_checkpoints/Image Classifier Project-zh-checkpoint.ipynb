{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开发 AI 应用\n",
    "\n",
    "未来，AI 算法在日常生活中的应用将越来越广泛。例如，你可能想要在智能手机应用中包含图像分类器。为此，在整个应用架构中，你将使用一个用成百上千个图像训练过的深度学习模型。未来的软件开发很大一部分将是使用这些模型作为应用的常用部分。\n",
    "\n",
    "在此项目中，你将训练一个图像分类器来识别不同的花卉品种。可以想象有这么一款手机应用，当你对着花卉拍摄时，它能够告诉你这朵花的名称。在实际操作中，你会训练此分类器，然后导出它以用在你的应用中。我们将使用[此数据集](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html)，其中包含 102 个花卉类别。你可以在下面查看几个示例。 \n",
    "\n",
    "<img src='assets/Flowers.png' width=500px>\n",
    "\n",
    "该项目分为多个步骤：\n",
    "\n",
    "* 加载和预处理图像数据集\n",
    "* 用数据集训练图像分类器\n",
    "* 使用训练的分类器预测图像内容\n",
    "\n",
    "我们将指导你完成每一步，你将用 Python 实现这些步骤。\n",
    "\n",
    "完成此项目后，你将拥有一个可以用任何带标签图像的数据集进行训练的应用。你的网络将学习花卉，并成为一个命令行应用。但是，你对新技能的应用取决于你的想象力和构建数据集的精力。例如，想象有一款应用能够拍摄汽车，告诉你汽车的制造商和型号，然后查询关于该汽车的信息。构建你自己的数据集并开发一款新型应用吧。\n",
    "\n",
    "首先，导入你所需的软件包。建议在代码开头导入所有软件包。当你创建此 notebook 时，如果发现你需要导入某个软件包，确保在开头导入该软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 0.4.1\n"
     ]
    }
   ],
   "source": [
    "# Imports here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import models, transforms, datasets\n",
    "import os\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "\n",
    "在此项目中，你将使用 `torchvision` 加载数据（[文档](http://pytorch.org/docs/master/torchvision/transforms.html#)）。数据应该和此 notebook 一起包含在内，否则你可以[在此处下载数据](https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz)。数据集分成了三部分：训练集、验证集和测试集。对于训练集，你需要变换数据，例如随机缩放、剪裁和翻转。这样有助于网络泛化，并带来更好的效果。你还需要确保将输入数据的大小调整为 224x224 像素，因为预训练的网络需要这么做。\n",
    "\n",
    "验证集和测试集用于衡量模型对尚未见过的数据的预测效果。对此步骤，你不需要进行任何缩放或旋转变换，但是需要将图像剪裁到合适的大小。\n",
    "\n",
    "对于所有三个数据集，你都需要将均值和标准差标准化到网络期望的结果。均值为 `[0.485, 0.456, 0.406]`，标准差为 `[0.229, 0.224, 0.225]`。这样使得每个颜色通道的值位于 -1 到 1 之间，而不是 0 到 1 之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "valid_dir = 'valid'\n",
    "test_dir = 'test'\n",
    "data_dir = 'flowers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes:102\n",
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), \n",
    "                                          data_transforms[x]) for x in ['train', 'valid', 'test']}\n",
    "\n",
    "class_indexes = image_datasets['train'].class_to_idx\n",
    "print('number of classes:{}'.format(len(class_indexes)))\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64, shuffle=True) \n",
    "               for x in ['train', 'valid', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'valid', 'test']}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签映射\n",
    "\n",
    "你还需要加载从类别标签到类别名称的映射。你可以在文件 `cat_to_name.json` 中找到此映射。它是一个 JSON 对象，可以使用 [`json` 模块](https://docs.python.org/2/library/json.html)读取它。这样可以获得一个从整数编码的类别到实际花卉名称的映射字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建和训练分类器\n",
    "\n",
    "数据准备好后，就开始构建和训练分类器了。和往常一样，你应该使用 `torchvision.models` 中的某个预训练模型获取图像特征。使用这些特征构建和训练新的前馈分类器。\n",
    "\n",
    "这部分将由你来完成。如果你想与他人讨论这部分，欢迎与你的同学讨论！你还可以在论坛上提问或在工作时间内咨询我们的课程经理和助教导师。\n",
    "\n",
    "请参阅[审阅标准](https://review.udacity.com/#!/rubrics/1663/view)，了解如何成功地完成此部分。你需要执行以下操作：\n",
    "\n",
    "* 加载[预训练的网络](http://pytorch.org/docs/master/torchvision/models.html)（如果你需要一个起点，推荐使用 VGG 网络，它简单易用）\n",
    "* 使用 ReLU 激活函数和丢弃定义新的未训练前馈网络作为分类器\n",
    "* 使用反向传播训练分类器层，并使用预训练的网络获取特征\n",
    "* 跟踪验证集的损失和准确率，以确定最佳超参数\n",
    "\n",
    "我们在下面为你留了一个空的单元格，但是你可以使用多个单元格。建议将问题拆分为更小的部分，并单独运行。检查确保每部分都达到预期效果，然后再完成下个部分。你可能会发现，当你实现每部分时，可能需要回去修改之前的代码，这很正常！\n",
    "\n",
    "训练时，确保仅更新前馈网络的权重。如果一切构建正确的话，验证准确率应该能够超过 70%。确保尝试不同的超参数（学习速率、分类器中的单元、周期等），寻找最佳模型。保存这些超参数并用作项目下个部分的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build and train your network\n",
    "# 导出特征向量\n",
    "# 模型：Densenet-121，ResNet-18，VGG-19\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "class VGG19FineTune(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19FineTune, self).__init__()\n",
    "        self.features = vgg19.features\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        # only change classifer layer\n",
    "        self.classifers = nn.Sequential(\n",
    "            *list(vgg19.classifier.children())[:-1], \n",
    "            nn.Linear(in_features=4096, out_features=len(image_datasets['train'].classes), bias=True))\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifers(x)\n",
    "        return x\n",
    "\n",
    "vgg19Tune = VGG19FineTune()\n",
    "# print(vgg19Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)\n",
    "class Resnet50FineTune(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet50FineTune, self).__init__()\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-1])\n",
    "        for param in self.parameters():\n",
    "            param.require_grad = False\n",
    "        # only change classifer layer\n",
    "        self.classifiers = nn.Linear(in_features=2048, out_features=len(image_datasets['train'].classes), bias=True)\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifiers(x)\n",
    "        return x\n",
    "\n",
    "resnet50Tune = Resnet50FineTune()\n",
    "# print(resnet50Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "densenet121 = models.densenet121(pretrained=True)\n",
    "class Densenet121FineTune(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Densenet121FineTune, self).__init__()\n",
    "        self.features = densenet121.features\n",
    "        for param in self.parameters():\n",
    "            param.require_grad = False\n",
    "        # only change classifer layer\n",
    "        self.classifiers = nn.Linear(in_features=1024, out_features=len(image_datasets['train'].classes), bias=True)\n",
    "    def forward(self, x):\n",
    "        # copy from Densenet implementation in pytorch/vision\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n",
    "        out = self.classifiers(out)\n",
    "        return out\n",
    "\n",
    "densenet121Tune = Densenet121FineTune()\n",
    "# print(densenet121Tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    print_every = 3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('epoch {}/{}'.format(epoch, num_epochs-1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = Variable(inputs)\n",
    "                labels = Variable(labels)\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            print('{} loss: {:.4f} acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print('best acc {:.4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/24\n",
      "----------\n",
      "train loss: 2.2922 acc: 0.4469\n",
      "valid loss: 1.2472 acc: 0.6601\n",
      "\n",
      "epoch 1/24\n",
      "----------\n",
      "train loss: 1.2694 acc: 0.6685\n",
      "valid loss: 1.0699 acc: 0.7054\n",
      "\n",
      "epoch 2/24\n",
      "----------\n",
      "train loss: 1.0704 acc: 0.7074\n",
      "valid loss: 0.9288 acc: 0.7408\n",
      "\n",
      "epoch 3/24\n",
      "----------\n",
      "train loss: 0.9940 acc: 0.7215\n",
      "valid loss: 0.8740 acc: 0.7567\n",
      "\n",
      "epoch 4/24\n",
      "----------\n",
      "train loss: 0.8964 acc: 0.7543\n",
      "valid loss: 0.8171 acc: 0.7800\n",
      "\n",
      "epoch 5/24\n",
      "----------\n",
      "train loss: 0.8426 acc: 0.7674\n",
      "valid loss: 0.8264 acc: 0.7726\n",
      "\n",
      "epoch 6/24\n",
      "----------\n",
      "train loss: 0.7908 acc: 0.7811\n",
      "valid loss: 0.7643 acc: 0.7897\n",
      "\n",
      "epoch 7/24\n",
      "----------\n",
      "train loss: 0.7617 acc: 0.7819\n",
      "valid loss: 0.8179 acc: 0.7787\n",
      "\n",
      "epoch 8/24\n",
      "----------\n",
      "train loss: 0.7159 acc: 0.7999\n",
      "valid loss: 0.7677 acc: 0.7861\n",
      "\n",
      "epoch 9/24\n",
      "----------\n",
      "train loss: 0.7561 acc: 0.7908\n",
      "valid loss: 0.7491 acc: 0.7971\n",
      "\n",
      "epoch 10/24\n",
      "----------\n",
      "train loss: 0.6905 acc: 0.8028\n",
      "valid loss: 0.7293 acc: 0.8081\n",
      "\n",
      "epoch 11/24\n",
      "----------\n",
      "train loss: 0.6681 acc: 0.8167\n",
      "valid loss: 0.7608 acc: 0.7971\n",
      "\n",
      "epoch 12/24\n",
      "----------\n",
      "train loss: 0.6548 acc: 0.8231\n",
      "valid loss: 0.7094 acc: 0.8166\n",
      "\n",
      "epoch 13/24\n",
      "----------\n",
      "train loss: 0.6500 acc: 0.8193\n",
      "valid loss: 0.7168 acc: 0.8032\n",
      "\n",
      "epoch 14/24\n",
      "----------\n",
      "train loss: 0.6400 acc: 0.8246\n",
      "valid loss: 0.6832 acc: 0.8154\n",
      "\n",
      "epoch 15/24\n",
      "----------\n",
      "train loss: 0.6168 acc: 0.8274\n",
      "valid loss: 0.7226 acc: 0.8178\n",
      "\n",
      "epoch 16/24\n",
      "----------\n",
      "train loss: 0.6093 acc: 0.8320\n",
      "valid loss: 0.7288 acc: 0.8056\n",
      "\n",
      "epoch 17/24\n",
      "----------\n",
      "train loss: 0.5932 acc: 0.8375\n",
      "valid loss: 0.6851 acc: 0.8178\n",
      "\n",
      "epoch 18/24\n",
      "----------\n",
      "train loss: 0.5859 acc: 0.8451\n",
      "valid loss: 0.6863 acc: 0.8289\n",
      "\n",
      "epoch 19/24\n",
      "----------\n",
      "train loss: 0.5608 acc: 0.8472\n",
      "valid loss: 0.6471 acc: 0.8313\n",
      "\n",
      "epoch 20/24\n",
      "----------\n",
      "train loss: 0.5326 acc: 0.8518\n",
      "valid loss: 0.7426 acc: 0.7995\n",
      "\n",
      "epoch 21/24\n",
      "----------\n",
      "train loss: 0.5488 acc: 0.8492\n",
      "valid loss: 0.6881 acc: 0.8166\n",
      "\n",
      "epoch 22/24\n",
      "----------\n",
      "train loss: 0.5248 acc: 0.8544\n",
      "valid loss: 0.7330 acc: 0.8227\n",
      "\n",
      "epoch 23/24\n",
      "----------\n",
      "train loss: 0.5138 acc: 0.8559\n",
      "valid loss: 0.6696 acc: 0.8264\n",
      "\n",
      "epoch 24/24\n",
      "----------\n",
      "train loss: 0.5249 acc: 0.8573\n",
      "valid loss: 0.7520 acc: 0.8044\n",
      "\n",
      "best acc 0.8313\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning vgg model\n",
    "vgg19Tune.cuda()\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg19Tune.classifers.parameters(), lr=0.01, momentum=0.9)\n",
    "vgg19TuneBest = train_model(vgg19Tune, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/24\n",
      "----------\n",
      "train loss: 2.9398 acc: 0.4055\n",
      "valid loss: 1.8984 acc: 0.6112\n",
      "\n",
      "epoch 1/24\n",
      "----------\n",
      "train loss: 1.2620 acc: 0.7712\n",
      "valid loss: 1.1981 acc: 0.7751\n",
      "\n",
      "epoch 2/24\n",
      "----------\n",
      "train loss: 0.8758 acc: 0.8405\n",
      "valid loss: 0.9633 acc: 0.8081\n",
      "\n",
      "epoch 3/24\n",
      "----------\n",
      "train loss: 0.6812 acc: 0.8726\n",
      "valid loss: 0.8743 acc: 0.8215\n",
      "\n",
      "epoch 4/24\n",
      "----------\n",
      "train loss: 0.5955 acc: 0.8878\n",
      "valid loss: 0.7927 acc: 0.8337\n",
      "\n",
      "epoch 5/24\n",
      "----------\n",
      "train loss: 0.5302 acc: 0.8973\n",
      "valid loss: 0.7719 acc: 0.8337\n",
      "\n",
      "epoch 6/24\n",
      "----------\n",
      "train loss: 0.4988 acc: 0.8999\n",
      "valid loss: 0.7454 acc: 0.8350\n",
      "\n",
      "epoch 7/24\n",
      "----------\n",
      "train loss: 0.4623 acc: 0.9063\n",
      "valid loss: 0.6821 acc: 0.8509\n",
      "\n",
      "epoch 8/24\n",
      "----------\n",
      "train loss: 0.4163 acc: 0.9158\n",
      "valid loss: 0.6735 acc: 0.8411\n",
      "\n",
      "epoch 9/24\n",
      "----------\n",
      "train loss: 0.4015 acc: 0.9180\n",
      "valid loss: 0.6349 acc: 0.8496\n",
      "\n",
      "epoch 10/24\n",
      "----------\n",
      "train loss: 0.3903 acc: 0.9209\n",
      "valid loss: 0.6148 acc: 0.8496\n",
      "\n",
      "epoch 11/24\n",
      "----------\n",
      "train loss: 0.3805 acc: 0.9170\n",
      "valid loss: 0.6174 acc: 0.8496\n",
      "\n",
      "epoch 12/24\n",
      "----------\n",
      "train loss: 0.3685 acc: 0.9165\n",
      "valid loss: 0.5901 acc: 0.8484\n",
      "\n",
      "epoch 13/24\n",
      "----------\n",
      "train loss: 0.3452 acc: 0.9222\n",
      "valid loss: 0.5800 acc: 0.8582\n",
      "\n",
      "epoch 14/24\n",
      "----------\n",
      "train loss: 0.3408 acc: 0.9293\n",
      "valid loss: 0.5616 acc: 0.8594\n",
      "\n",
      "epoch 15/24\n",
      "----------\n",
      "train loss: 0.3310 acc: 0.9287\n",
      "valid loss: 0.5582 acc: 0.8619\n",
      "\n",
      "epoch 16/24\n",
      "----------\n",
      "train loss: 0.3222 acc: 0.9280\n",
      "valid loss: 0.5505 acc: 0.8509\n",
      "\n",
      "epoch 17/24\n",
      "----------\n",
      "train loss: 0.3124 acc: 0.9342\n",
      "valid loss: 0.5364 acc: 0.8557\n",
      "\n",
      "epoch 18/24\n",
      "----------\n",
      "train loss: 0.3060 acc: 0.9348\n",
      "valid loss: 0.5395 acc: 0.8631\n",
      "\n",
      "epoch 19/24\n",
      "----------\n",
      "train loss: 0.2920 acc: 0.9342\n",
      "valid loss: 0.5268 acc: 0.8619\n",
      "\n",
      "epoch 20/24\n",
      "----------\n",
      "train loss: 0.2922 acc: 0.9344\n",
      "valid loss: 0.5196 acc: 0.8643\n",
      "\n",
      "epoch 21/24\n",
      "----------\n",
      "train loss: 0.2728 acc: 0.9389\n",
      "valid loss: 0.5185 acc: 0.8606\n",
      "\n",
      "epoch 22/24\n",
      "----------\n",
      "train loss: 0.2824 acc: 0.9359\n",
      "valid loss: 0.5097 acc: 0.8606\n",
      "\n",
      "epoch 23/24\n",
      "----------\n",
      "train loss: 0.2858 acc: 0.9367\n",
      "valid loss: 0.5232 acc: 0.8557\n",
      "\n",
      "epoch 24/24\n",
      "----------\n",
      "train loss: 0.2700 acc: 0.9406\n",
      "valid loss: 0.4790 acc: 0.8692\n",
      "\n",
      "best acc 0.8692\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning resnet model \n",
    "resnet50Tune.cuda()\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50Tune.classifiers.parameters(), lr=0.01, momentum=0.9)\n",
    "resnet50TuneBest = train_model(resnet50Tune, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/24\n",
      "----------\n",
      "train loss: 2.8114 acc: 0.4408\n",
      "valid loss: 1.7365 acc: 0.6907\n",
      "\n",
      "epoch 1/24\n",
      "----------\n",
      "train loss: 1.1170 acc: 0.8057\n",
      "valid loss: 1.1418 acc: 0.7836\n",
      "\n",
      "epoch 2/24\n",
      "----------\n",
      "train loss: 0.7441 acc: 0.8698\n",
      "valid loss: 0.9398 acc: 0.8068\n",
      "\n",
      "epoch 3/24\n",
      "----------\n",
      "train loss: 0.6044 acc: 0.8906\n",
      "valid loss: 0.8321 acc: 0.8117\n",
      "\n",
      "epoch 4/24\n",
      "----------\n",
      "train loss: 0.5148 acc: 0.9034\n",
      "valid loss: 0.7699 acc: 0.8276\n",
      "\n",
      "epoch 5/24\n",
      "----------\n",
      "train loss: 0.4682 acc: 0.9112\n",
      "valid loss: 0.7141 acc: 0.8325\n",
      "\n",
      "epoch 6/24\n",
      "----------\n",
      "train loss: 0.4299 acc: 0.9113\n",
      "valid loss: 0.6993 acc: 0.8460\n",
      "\n",
      "epoch 7/24\n",
      "----------\n",
      "train loss: 0.3928 acc: 0.9225\n",
      "valid loss: 0.6433 acc: 0.8496\n",
      "\n",
      "epoch 8/24\n",
      "----------\n",
      "train loss: 0.3670 acc: 0.9278\n",
      "valid loss: 0.6287 acc: 0.8460\n",
      "\n",
      "epoch 9/24\n",
      "----------\n",
      "train loss: 0.3693 acc: 0.9222\n",
      "valid loss: 0.6125 acc: 0.8606\n",
      "\n",
      "epoch 10/24\n",
      "----------\n",
      "train loss: 0.3403 acc: 0.9284\n",
      "valid loss: 0.5959 acc: 0.8472\n",
      "\n",
      "epoch 11/24\n",
      "----------\n",
      "train loss: 0.3289 acc: 0.9309\n",
      "valid loss: 0.5858 acc: 0.8447\n",
      "\n",
      "epoch 12/24\n",
      "----------\n",
      "train loss: 0.3272 acc: 0.9315\n",
      "valid loss: 0.5592 acc: 0.8667\n",
      "\n",
      "epoch 13/24\n",
      "----------\n",
      "train loss: 0.3123 acc: 0.9315\n",
      "valid loss: 0.5673 acc: 0.8631\n",
      "\n",
      "epoch 14/24\n",
      "----------\n",
      "train loss: 0.3014 acc: 0.9359\n",
      "valid loss: 0.5605 acc: 0.8631\n",
      "\n",
      "epoch 15/24\n",
      "----------\n",
      "train loss: 0.2752 acc: 0.9388\n",
      "valid loss: 0.5405 acc: 0.8496\n",
      "\n",
      "epoch 16/24\n",
      "----------\n",
      "train loss: 0.2663 acc: 0.9438\n",
      "valid loss: 0.5288 acc: 0.8582\n",
      "\n",
      "epoch 17/24\n",
      "----------\n",
      "train loss: 0.2592 acc: 0.9458\n",
      "valid loss: 0.5260 acc: 0.8704\n",
      "\n",
      "epoch 18/24\n",
      "----------\n",
      "train loss: 0.2694 acc: 0.9418\n",
      "valid loss: 0.5229 acc: 0.8631\n",
      "\n",
      "epoch 19/24\n",
      "----------\n",
      "train loss: 0.2461 acc: 0.9455\n",
      "valid loss: 0.5003 acc: 0.8704\n",
      "\n",
      "epoch 20/24\n",
      "----------\n",
      "train loss: 0.2634 acc: 0.9422\n",
      "valid loss: 0.5015 acc: 0.8667\n",
      "\n",
      "epoch 21/24\n",
      "----------\n",
      "train loss: 0.2351 acc: 0.9509\n",
      "valid loss: 0.4928 acc: 0.8716\n",
      "\n",
      "epoch 22/24\n",
      "----------\n",
      "train loss: 0.2415 acc: 0.9443\n",
      "valid loss: 0.4822 acc: 0.8753\n",
      "\n",
      "epoch 23/24\n",
      "----------\n",
      "train loss: 0.2286 acc: 0.9493\n",
      "valid loss: 0.4706 acc: 0.8790\n",
      "\n",
      "epoch 24/24\n",
      "----------\n",
      "train loss: 0.2332 acc: 0.9493\n",
      "valid loss: 0.4730 acc: 0.8814\n",
      "\n",
      "best acc 0.8814\n"
     ]
    }
   ],
   "source": [
    "densenet121Tune.cuda()\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(densenet121Tune.classifiers.parameters(), lr=0.01, momentum=0.9)\n",
    "densenet121TuneBest = train_model(densenet121Tune, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试网络\n",
    "\n",
    "建议使用网络在训练或验证过程中从未见过的测试数据测试训练的网络。这样，可以很好地判断模型预测全新图像的效果。用网络预测测试图像，并测量准确率，就像验证过程一样。如果模型训练良好的话，你应该能够达到大约 70% 的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "            \n",
    "    for inputs, labels in dataloaders['test']:\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "                \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "                \n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "    epoch_loss = running_loss / dataset_sizes['test']\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes['test']\n",
    "    print('test loss: {:.4f} acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.7380 acc: 0.7827\n"
     ]
    }
   ],
   "source": [
    "# TODO: Do validation on the test set\n",
    "test_model(vgg19TuneBest, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.5180 acc: 0.8669\n"
     ]
    }
   ],
   "source": [
    "test_model(resnet50TuneBest, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.4922 acc: 0.8669\n"
     ]
    }
   ],
   "source": [
    "test_model(densenet121TuneBest, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存检查点\n",
    "\n",
    "训练好网络后，保存模型，以便稍后加载它并进行预测。你可能还需要保存其他内容，例如从类别到索引的映射，索引是从某个图像数据集中获取的：`image_datasets['train'].class_to_idx`。你可以将其作为属性附加到模型上，这样稍后推理会更轻松。\n",
    "\n",
    "注意，稍后你需要完全重新构建模型，以便用模型进行推理。确保在检查点中包含你所需的任何信息。如果你想加载模型并继续训练，则需要保存周期数量和优化器状态 `optimizer.state_dict`。你可能需要在下面的下个部分使用训练的模型，因此建议立即保存它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Save the checkpoint\n",
    "checkpoint = {\n",
    "    'vgg19': vgg19TuneBest.state_dict(),\n",
    "    'resnet50': resnet50TuneBest.state_dict(),\n",
    "    'densenet': densenet121TuneBest.state_dict(),\n",
    "    'class_to_idx': image_datasets['train'].class_to_idx,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载检查点\n",
    "\n",
    "此刻，建议写一个可以加载检查点并重新构建模型的函数。这样的话，你可以回到此项目并继续完善它，而不用重新训练网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function that loads a checkpoint and rebuilds the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 类别推理\n",
    "\n",
    "现在，你需要写一个使用训练的网络进行推理的函数。即你将向网络中传入一个图像，并预测图像中的花卉类别。写一个叫做 `predict` 的函数，该函数会接受图像和模型，然后返回概率在前 $K$ 的类别及其概率。应该如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，你需要处理输入图像，使其可以用于你的网络。\n",
    "\n",
    "## 图像处理\n",
    "\n",
    "你需要使用 `PIL` 加载图像（[文档](https://pillow.readthedocs.io/en/latest/reference/Image.html)）。建议写一个函数来处理图像，使图像可以作为模型的输入。该函数应该按照训练的相同方式处理图像。\n",
    "\n",
    "首先，调整图像大小，使最小的边为 256 像素，并保持宽高比。为此，可以使用 [`thumbnail`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) 或 [`resize`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) 方法。然后，你需要从图像的中心裁剪出 224x224 的部分。\n",
    "\n",
    "图像的颜色通道通常编码为整数 0-255，但是该模型要求值为浮点数 0-1。你需要变换值。使用 Numpy 数组最简单，你可以从 PIL 图像中获取，例如 `np_image = np.array(pil_image)`。\n",
    "\n",
    "和之前一样，网络要求图像按照特定的方式标准化。均值应标准化为 `[0.485, 0.456, 0.406]`，标准差应标准化为 `[0.229, 0.224, 0.225]`。你需要用每个颜色通道减去均值，然后除以标准差。\n",
    "\n",
    "最后，PyTorch 要求颜色通道为第一个维度，但是在 PIL 图像和 Numpy 数组中是第三个维度。你可以使用 [`ndarray.transpose`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.transpose.html)对维度重新排序。颜色通道必须是第一个维度，并保持另外两个维度的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # TODO: Process a PIL image for use in a PyTorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要检查你的项目，可以使用以下函数来转换 PyTorch 张量并将其显示在  notebook 中。如果 `process_image` 函数可行，用该函数运行输出应该会返回原始图像（但是剪裁掉的部分除外）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别预测\n",
    "\n",
    "可以获得格式正确的图像后 \n",
    "\n",
    "要获得前 $K$ 个值，在张量中使用 [`x.topk(k)`](http://pytorch.org/docs/master/torch.html#torch.topk)。该函数会返回前 `k` 个概率和对应的类别索引。你需要使用  `class_to_idx`（希望你将其添加到了模型中）将这些索引转换为实际类别标签，或者从用来加载数据的[ `ImageFolder`](https://pytorch.org/docs/master/torchvision/datasets.html?highlight=imagefolder#torchvision.datasets.ImageFolder)进行转换。确保颠倒字典\n",
    "\n",
    "同样，此方法应该接受图像路径和模型检查点，并返回概率和类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查运行状况\n",
    "\n",
    "你已经可以使用训练的模型做出预测，现在检查模型的性能如何。即使测试准确率很高，始终有必要检查是否存在明显的错误。使用 `matplotlib` 将前 5 个类别的概率以及输入图像绘制为条形图，应该如下所示：\n",
    "\n",
    "<img src='assets/inference_example.png' width=300px>\n",
    "\n",
    "你可以使用 `cat_to_name.json` 文件（应该之前已经在 notebook 中加载该文件）将类别整数编码转换为实际花卉名称。要将 PyTorch 张量显示为图像，请使用定义如下的 `imshow` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display an image along with the top 5 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(*list(vgg19.children())[:-1])\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        return x\n",
    "    \n",
    "class ResnetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResnetFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        return x\n",
    "\n",
    "class Densenet121FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Densenet121FeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(*list(densenet121.children())[:-1])\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
    "        return x\n",
    "    \n",
    "# print(vgg19)\n",
    "# extractor = ResnetFeatureExtractor()\n",
    "# extractor.cuda()\n",
    "# count = 0\n",
    "# for inputs, labels in dataloaders['train']:\n",
    "#     inputs = inputs.to(device)\n",
    "#     labels = inputs.to(device)\n",
    "#     outputs = extractor(inputs)\n",
    "#     # convert a 4-d tensor to 2-d tensor\n",
    "#     outputs = outputs.view(8, -1)\n",
    "#     print(outputs.shape)\n",
    "#     count += 1\n",
    "#     if count >= 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
